{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35789e36-03ee-40d9-92ee-8fe8eaf528a6",
   "metadata": {},
   "source": [
    "# Introduction of LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5f3b3e-0700-4dc1-9017-35afc8199c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e10c1bc-659d-497a-845c-9740246ddd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the API key\n",
    "os.environ['OPENAI_API_KEY'] = \"xxx\"\n",
    "\n",
    "# Now you can use the API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f48f7-f5ab-47d9-af03-e1049dc97a5a",
   "metadata": {},
   "source": [
    "## Chat Models: GPT-3.5 Turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc0b5625-8595-4f26-95bb-7b0d9b6c25ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada is a vast, multicultural country in North America known for its natural beauty, politeness, and strong commitment to human rights and multiculturalism.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()  \n",
    "\n",
    "# invoking the llm (running the prompt)\n",
    "output = llm.invoke('Explain Canada in one sentence.', model='gpt-4-turbo', temperature=0.1)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20b7ce54-46c9-43b5-8421-d2c15bf93c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加拿大是一个北美洲的国家，拥有广阔的土地、丰富的资源和多元的文化。\n"
     ]
    }
   ],
   "source": [
    "# using Chat Completions API Messages: System, Assistant and Human\n",
    "from langchain.schema import(\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in Chinese.'),\n",
    "    HumanMessage(content='Explain Canada in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fee876-1b9f-4f4c-aaad-d4ff22b2ad9f",
   "metadata": {},
   "source": [
    "## Caching LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee3f24ce-9028-464b-bc80-16d1a14572d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-memory cache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32a19f6b-7c81-4075-af67-9ae767cc7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ec9d42d-10ad-4d8c-9f17-8e05b361707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 2.64 ms, total: 18.4 ms\n",
      "Wall time: 615 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did the banana go to the doctor?\\n\\nBecause it wasn't peeling well!\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77dc4b82-ee33-4da2-83bd-9eb7afc41595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 429 μs, sys: 61 μs, total: 490 μs\n",
      "Wall time: 554 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nWhy did the banana go to the doctor?\\n\\nBecause it wasn't peeling well!\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2a9fbb4-9cc9-4612-bd86-b5634e4ae549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30298f51-3c1f-4521-8237-cd1e496ceec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 ms, sys: 2.99 ms, total: 22.9 ms\n",
      "Wall time: 622 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything. \""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d67d91f8-8964-4cce-abc1-43f64452b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.08 ms, sys: 5.13 ms, total: 13.2 ms\n",
      "Wall time: 12.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhy don't scientists trust atoms?\\n\\nBecause they make up everything. \""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1de5ac-b2f2-4764-9369-d05cedbac273",
   "metadata": {},
   "source": [
    "## LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "214d32a4-24c9-4c60-99b4-11cf30593e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "In the dead of night, under the glow\n",
      "Of the silvery moon, a raven flies low\n",
      "Its wings beat against the midnight sky\n",
      "As it calls out a haunting cry\n",
      "\n",
      "Chorus:\n",
      "Oh, moon and raven, dance in the night\n",
      "Their mysterious ways, hidden from sight\n",
      "Together they roam, in the darkness they thrive\n",
      "A bond unbreakable, they'll never divide\n",
      "\n",
      "Verse 2:\n",
      "The moon whispers secrets to the raven's ear\n",
      "As they soar through the heavens, without fear\n",
      "They're creatures of the night, wild and free\n",
      "A sight to behold, for all to see\n",
      "\n",
      "Chorus:\n",
      "Oh, moon and raven, dance in the night\n",
      "Their mysterious ways, hidden from sight\n",
      "Together they roam, in the darkness they thrive\n",
      "A bond unbreakable, they'll never divide\n",
      "\n",
      "Bridge:\n",
      "They're symbols of mystery, magic, and lore\n",
      "In their presence, you can't help but adore\n",
      "The moon and the raven, a sight so divine\n",
      "Their beauty and power, forever intertwine\n",
      "\n",
      "Chorus:\n",
      "Oh, moon and raven, dance in the night\n",
      "Their mysterious ways, hidden from sight\n",
      "Together they roam, in the darkness they thrive\n",
      "A bond unbreakable, they'll never divide\n",
      "\n",
      "Outro:\n",
      "So when you see them together, high above\n",
      "Remember the moon and the raven, united in love\n",
      "In the darkness they reign, ruling the night\n",
      "A magical duo, a mesmerizing sight.\n"
     ]
    }
   ],
   "source": [
    "# the default mode is llm will return the complete task then return\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81131e19-0b9a-46c4-a1f3-3ce91d9ce34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "The moon hangs high in the midnight sky\n",
      "Casting shadows on the land below\n",
      "A raven calls out in the dead of night\n",
      "Its cry echoing through the darkened world\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, spirits of the night\n",
      "Dancing in the shadows, bathed in silver light\n",
      "Mysteries unfolding in the pale moon's glow\n",
      "Moon and raven, forever intertwined\n",
      "\n",
      "Verse 2:\n",
      "The raven perches on a gnarled old tree\n",
      "Its eyes gleaming with ancient wisdom\n",
      "It watches as the moon rises higher\n",
      "A silent sentinel in the velvet sky\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, spirits of the night\n",
      "Dancing in the shadows, bathed in silver light\n",
      "Mysteries unfolding in the pale moon's glow\n",
      "Moon and raven, forever intertwined\n",
      "\n",
      "Bridge:\n",
      "They are the guardians of the night\n",
      "A pair of souls forever bound\n",
      "In the darkness they find their light\n",
      "In the silence, their secrets are found\n",
      "\n",
      "Chorus:\n",
      "Moon and raven, spirits of the night\n",
      "Dancing in the shadows, bathed in silver light\n",
      "Mysteries unfolding in the pale moon's glow\n",
      "Moon and raven, forever intertwined\n",
      "\n",
      "Outro:\n",
      "As the night fades and the dawn breaks\n",
      "The moon and raven bid farewell\n",
      "But they will return, in the dead of night\n",
      "To reign over the darkness once more."
     ]
    }
   ],
   "source": [
    "# enable streaming\n",
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f077f52-1e23-4c0d-b321-66c7f125d2e8",
   "metadata": {},
   "source": [
    "## PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "72d3d972-14db-493e-b3b1-2b94ad1383f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an experienced virologist.\\nWrite a few sentences about the following virus \"zita\" in Chinese.\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a template for the prompt\n",
    "template = '''\n",
    "You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.\n",
    "'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Fill in the variable: virus and language\n",
    "prompt = prompt_template.format(virus='zita', language='Chinese')\n",
    "prompt  # Returns the generated prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c06be26-8c13-4dbf-b4de-de8d5bd54fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Zita\" 病毒是一种新型冠状病毒，最初在动物中发现，后来传播给人类。这种病毒具有高度传染性，可引起呼吸道疾病和肺部感染。目前尚无特效药物治疗，预防措施包括保持良好的个人卫生和避免密切接触感染者。\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ced5517-5468-4492-ad51-ac0361006f51",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "edfcd247-1699-4d5e-a007-2c5560ca19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n"
     ]
    }
   ],
   "source": [
    "# chat promopt tenplates are for conversations\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "print(messages)  # Outputs the formatted chat messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "610e86b1-dc47-4606-b2a5-ea3e78ddf120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\n",
      "            \"name\": \"China\",\n",
      "            \"population\": \"1,439,323,776\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"India\",\n",
      "            \"population\": \"1,380,004,385\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"United States\",\n",
      "            \"population\": \"331,002,651\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Indonesia\",\n",
      "            \"population\": \"273,523,615\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Pakistan\",\n",
      "            \"population\": \"220,892,340\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748f081-8328-4fba-83e4-a0c11dbc031b",
   "metadata": {},
   "source": [
    "## Simple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b3a46eb-ca0a-45ff-b374-6531d67ce855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an experience virologist.\n",
      "Write a few sentences about the following virus \"zita\" in Chinese.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for complex conversations, we need chains to combine steps/components \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "# import llmchain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experience virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# create a simple chain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = chain.invoke({'virus': 'zita', 'language': 'Chinese'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e4858b92-c349-45e9-8e78-4c36c552573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virus': 'zita', 'language': 'Chinese', 'text': '\"Zita\"病毒是一种新型冠状病毒，最初在中国武汉市被发现。这种病毒能够引起呼吸道感染，导致发热、咳嗽和呼吸困难等症状。目前全球范围内都在努力应对这种病毒的传播。'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5df8c93a-8b14-4285-833a-dfbef2640367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Country:  China\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of China?. List the top 3 places to visit in that city. Use bullet points\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of China is Beijing.\n",
      "\n",
      "- The Great Wall of China\n",
      "- The Forbidden City\n",
      "- The Temple of Heaven\n"
     ]
    }
   ],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# enter a counrty first\n",
    "country = input('Enter Country: ')\n",
    "\n",
    "# Invoke the chain with specific virus and language values\n",
    "output = chain.invoke(country)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a3e18-f429-43c1-addf-bef0811c3991",
   "metadata": {},
   "source": [
    "## Sequential Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d38ffde-dc55-4518-b259-691f73fa3118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure! Here is a simple implementation of linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    X = np.array(X)\n",
      "    y = np.array(y)\n",
      "    \n",
      "    n = X.shape[0]\n",
      "    m = X.shape[1]\n",
      "    \n",
      "    X_b = np.c_[np.ones((n, 1)), X]  # add bias term\n",
      "    \n",
      "    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)  # calculate theta\n",
      "    \n",
      "    return theta\n",
      "\n",
      "# Example usage\n",
      "X = np.array([[1, 2], [2, 3], [3, 4]])\n",
      "y = np.array([2, 3, 4])\n",
      "\n",
      "theta = linear_regression(X, y)\n",
      "print(theta)\n",
      "```\n",
      "\n",
      "This function takes in a matrix X (features) and a vector y (target values) and calculates the optimal parameters theta using the normal equation. The function returns the parameters theta that minimize the mean squared error between the predicted values and the actual values.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe Python function `linear_regression` demonstrates a fundamental algorithm in machine learning and statistics: Linear Regression. This algorithm aims to find a linear relationship between a dependent variable `y` and one or more independent variables given in matrix `X`. The relationship is defined by parameters often denoted as theta (θ), which the function calculates through what's known as the **Normal Equation Method**. Here's a detailed step-by-step explanation of how the function operates:\n",
      "\n",
      "1. **Importing NumPy**: ```import numpy as np``` - The function starts by importing the `numpy` library as `np`. NumPy is a powerful numeric library in Python that allows for efficient operations on arrays and matrices, which are crucial for linear algebra calculations involved in linear regression.\n",
      "\n",
      "2. **Function Definition**: The function `linear_regression` is defined with two parameters:\n",
      "   - `X`: A matrix where each row represents a sample and each column represents a feature/Independent Variable.\n",
      "   - `y`: A vector where each element is the dependent/target variable for each sample.\n",
      "\n",
      "3. **Array Conversion**: Although the inputs are presumed to be numpy arrays, the function explicitly converts `X` and `y` to numpy arrays to ensure compatibility with NumPy's matrix operations.\n",
      "\n",
      "4. **Dimensions Calculation**:\n",
      "   - `n` is assigned the number of rows in `X` (`n = X.shape[0]`), representing the total number of samples.\n",
      "   - `m` is assigned the number of features (`m = X.shape[1]`).\n",
      "\n",
      "5. **Bias Addition**:\n",
      "   - `X_b = np.c_[np.ones((n, 1)), X]` - Since linear regression models inherently calculate `y = θ0 + θ1*X1 + θ2*X2 + ... + θm*Xm`, a bias term (θ0 - also referred to as the intercept) is needed. This step prepends a column of ones to `X`, effectively adding a bias feature that will always be multiplied by one, to accommodate for this term directly in the parameter vector θ.\n",
      "\n",
      "6. **Calculating Theta (θ)**:\n",
      "   - `theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)` - This formula represents the Normal Equation: θ = (XᵀX)⁻¹Xᵀy. Here's a breakdown:\n",
      "     - `X_b.T`: Transposes the bias-included feature matrix `X_b`, flipping rows and columns.\n",
      "     - `X_b.T.dot(X_b)`: Performs matrix multiplication of `X_b.T` with `X_b`, yielding a square matrix.\n",
      "     - `np.linalg.inv(...)`: Calculates the inverse of the resulting square matrix.\n",
      "     - `.dot(X_b.T)`: Multiplies the inverse with the transposed `X_b` again.\n",
      "     - `.dot(y)`: Finally, multiplies the previous result with `y` to compute `θ`, the parameter vector that minimizes the cost function (mean squared error).\n",
      "\n",
      "7. **Function Return**: The function returns `theta`, the vector of calculated parameters including the bias term (`θ0`) and the weights/slopes for each feature (`θ1`, `θ2`, ..., `θm`).\n",
      "\n",
      "8. **Example Usage**: \n",
      "   The code block below the function demonstrates how to use the `linear_regression` function with a simple example. Here, `X` is defined as a numpy array comprising of sample features, and `y` as a corresponding array of target values. The function is then invoked with these arrays, and the resulting `theta` is printed, showing the calculated parameters fitting the linear model to the provided data best.\n",
      "\n",
      "In essence, this Python code snippet encapsulates the entirety of performing a basic linear regression analysis, providing valuable insights into both the implementation and fundamental understanding of one of the core algorithms in predictive modeling and machine learning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for more complex task\n",
    "# SimpleSequentialChain - answer as next chain's input\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
    "\n",
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e791fd0-7a7f-42e5-9331-ff576eb67e49",
   "metadata": {},
   "source": [
    "# LangChain Agents in Action: Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dfbbdc0f-1a71-49b8-809c-482b9376e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3452c7c1-6574-4d34-8ea8-53999872f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[13, 26, 39, 52, 65, 78, 91]\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7161305c-21eb-4b70-9793-91effd43cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo solve this, I need to calculate the factorial of 12 first, then find its square root, and finally format the result to display it with 4 decimal points. I can use the `math` module in Python to accomplish this, as it provides both `factorial` and `sqrt` functions.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that the `math` module is imported, I can use it to calculate the factorial of 12.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: factorial_12 = math.factorial(12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWith the factorial of 12 calculated, I can now proceed to find its square root.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: sqrt_factorial_12 = math.sqrt(factorial_12)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have the square root of the factorial of 12, I need to format it to display with 4 decimal points.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: print(f\"{sqrt_factorial_12:.4f}\")\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 21886.1052\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
       " 'output': '21886.1052'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model with gpt-4-turbo and a temperature of 0\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "\n",
    "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "prompt = 'Calculate the square root of the factorial of 12 and display it with 4 decimal points'\n",
    "agent_executor.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96eae22d-7506-4d45-b6be-4fc07c7ae389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
      "Action: Python_REPL\n",
      "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 146306.05007233328\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1577f3d5-6b6c-40c5-a1ff-7341f7f0aa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the answer to 5.1 ** 7.3?', 'output': '146306.05007233328'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2eedb-6de4-4115-a93f-7b208abf5069",
   "metadata": {},
   "source": [
    "## LangChain Tools: DuckDuckGo and Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "838290aa-169f-4793-ba2d-a688c282ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckduckgo\n",
    "#!pip install -q duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8d8dc757-07db-4ece-918d-7b3a3f465345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, filter workings (recommended in production)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bd6a9c82-c643-444f-b611-8aad10892eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's the final day of the Summer Olympics and the USA and China have tied in the overall gold medal rankings from these 2024 Summer Olympics. While Team USA is far ahead in the total medal ... A complete look at which countries are leading the medal count at the 2024 Paris Olympics. With the Paris Olympics complete, the U.S. topped the total medal count and tied China for the most golds. China held a slight lead over the U.S. entering Sunday and claimed its final gold medal ... The contest for most gold medals at the Paris Olympics ended in a 40-40 tie between China and the United States. With the 2024 Paris Olympics about to close, the United States will end the Summer Games as the country that has won more medals than any other nation.\n"
     ]
    }
   ],
   "source": [
    "#  DuckDuckGo search component.\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "output = search.invoke('Which counrty has the most number of gold medal in 2024 Olympics in Paries')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf0536f3-5938-4b5f-b0b1-4e58c5e3b5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ccdb982-8a4b-4764-9670-4994cee86dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1d7d32a1-3930-495c-a98c-6748c71738eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: It's the final day of the Summer Olympics and the USA and China have tied in the overall gold medal rankings from these 2024 Summer Olympics. While Team USA is far ahead in the total medal ..., title: USA and China tie for most gold medals in the 2024 Summer Olympics - CNN, link: https://www.cnn.com/2024/08/11/sport/olympics-medal-rankings-spt-intl/index.html], [snippet: The U.S. won one more gold than it did in the Tokyo Games three years ago, and America has gone plus-three in its silver tally, up from 41, and far surpassed the previous number of bronze medals ..., title: 2024 Olympics medal count: United States wins race, ties China with 40 ..., link: https://www.cbssports.com/olympics/news/2024-paris-olympics-medal-count-tracker-for-how-many-gold-silver-bronze-medals-usa-each-country-has-won/], [snippet: The United States again comfortably finished with the most medals at the 2020 Olympics in Tokyo, winning 113 overall and 39 gold, but the gap narrowed from 2016 when the U.S. won by a margin of 51 ..., title: Who is winning the Olympics? Updated 2024 medal standings, most gold ..., link: https://www.sportingnews.com/us/olympics/news/olympics-winner-2024-medal-standings/56aeaa329e2aa38435ac7b62], [snippet: As such, it would be a surprise to see the United States anywhere other than the top of the table by the time medals are awarded at the 2024 Paris Olympics. Sporting News is tracking the top medal ..., title: Olympics medal count 2024: Updated table of gold, silver, bronze medals ..., link: https://www.sportingnews.com/us/olympics/news/olympics-medal-count-2024-standings/d71c97e2053626649fad6312]\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "search = DuckDuckGoSearchResults()\n",
    "output = search.run('Which counrty has the most number of gold medal in 2024 Olympics in Paries')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "022b3ce7-3433-4b33-a7b7-88e7ee65d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "wrapper = DuckDuckGoSearchAPIWrapper(region='de-de', max_results=3, safesearch='moderate')\n",
    "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
    "output = search.run('2024 Paries Olympics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8011707c-42bb-4ef0-8ea6-d645d8a0d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[snippet: Record-breaking US 4x400m quartets bring Paris 2024 Olympics track and field programme to a thrilling finale. Ethiopia's late sub Tola wins marathon in Olympic record in brutal conditions over hugely challenging course. Paulino and Benjamin dazzle over one lap to win Olympic golds in Paris., title: Home | Paris 24 | Olympic Games - World Athletics, link: https://worldathletics.org/competitions/olympic-games/paris24], [snippet: Driven by the reforms of Olympic Agenda 2020, Paris 2024 set a precedent for future editions. Find out here how the IOC supported the Organising Committee and get access to key documents like the Host City Contract, qualification information, anti-doping and medical rules as well as media guidelines and News Access Rules., title: IOC Paris 2024 Gallery - Olympics.com, link: https://olympics.com/ioc/paris-2024], [snippet: Check out the full competition schedule for Paris 2024 on Olympics.com, with information on when every event will take place during the upcoming Olympic Games., title: Paris 2024 Olympic Games full schedule and day-by-day competitions, link: https://olympics.com/en/news/olympic-games-paris-2024-full-schedule-and-day-by-day-competitions], [snippet: Die Olympischen Sommerspiele 2024 (offiziell: Spiele der XXXIII. Olympiade, französisch Jeux olympiques d'été de 2024) fanden vom 26. Juli bis zum 11. August 2024 hauptsächlich in der französischen Hauptstadt Paris statt, mit einzelnen Wettbewerben in ganz Frankreich. Ab dem 24., title: Olympische Sommerspiele 2024 - Wikipedia, link: https://de.wikipedia.org/wiki/Olympische_Sommerspiele_2024]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3604d073-9147-4ee1-8dfa-8eea24e02974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snippet: Record-breaking US 4x400m quartets bring Paris 2024 Olympics track and field programme to a thrilling finale. Ethiopia's late sub Tola wins marathon in Olympic record in brutal conditions over hugely challenging course. Paulino and Benjamin dazzle over one lap to win Olympic golds in Paris.\n",
      "Title: Home | Paris 24 | Olympic Games - World Athletics\n",
      "Link: https://worldathletics.org/competitions/olympic-games/paris24\n",
      "\n",
      "--------------------------------------------------\n",
      "Snippet: Driven by the reforms of Olympic Agenda 2020, Paris 2024 set a precedent for future editions. Find out here how the IOC supported the Organising Committee and get access to key documents like the Host City Contract, qualification information, anti-doping and medical rules as well as media guidelines and News Access Rules.\n",
      "Title: IOC Paris 2024 Gallery - Olympics.com\n",
      "Link: https://olympics.com/ioc/paris-2024\n",
      "\n",
      "--------------------------------------------------\n",
      "Snippet: Check out the full competition schedule for Paris 2024 on Olympics.com, with information on when every event will take place during the upcoming Olympic Games.\n",
      "Title: Paris 2024 Olympic Games full schedule and day-by-day competitions\n",
      "Link: https://olympics.com/en/news/olympic-games-paris-2024-full-schedule-and-day-by-day-competitions\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)\\],'\n",
    "matches = re.findall(pattern, output, re.DOTALL)\n",
    "\n",
    "for snippet, title, link in matches:\n",
    "    print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb15ac-5168-4e8c-9032-ec25f2b3126f",
   "metadata": {},
   "source": [
    "## Wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2df94590-e050-4fb3-b032-3ffd10e883f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d4b932f7-c0f0-4458-8bfa-71012805d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ed314447-b96d-44e9-aa0d-90d337b3e910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Llama (language model)\\nSummary: Llama (acronym for Large Language Model Meta AI, and formerly stylized as LLaMA) is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The latest version is Llama 3.1, released in July 2024.\\nModel weights for the first version of Llama were made available to the research community under a non-commercial license, and access was granted on a case-by-case basis. Unauthorized copies of the model were shared via BitTorrent. In response, Meta AI issued DMCA takedown requests against repositories sharing the link on GitHub. Subsequent versions of Llama were made accessible outside academia and released under licenses that permitted some commercial use. Llama models are trained at different parameter sizes, ranging between 7B and 405B. Originally, Llama was only available as a foundation model. Starting with Llama 2, Meta AI started releasing instruction fine-tuned versions alongside foundation models.\\nAlongside the release of Llama 3, Meta added virtual assistant features to Facebook and WhatsApp in select regions, and a standalone website. Both services use a Llama 3 model.\\n\\n'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=10000)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.invoke({'query': 'Llama3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a68eb8f5-840e-4a57-aebc-6a14b7022261",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = wiki.invoke(\"Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "935d25fe-2ef5-410d-90ef-20747609aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Toronto\n",
      "Summary: Toronto is the most populous city in Canada and the capital city of the Canadian province of Ontario. With a population of 2,794,356 in 2021, it is the fourth-most populous city in North America. The city is the anchor of the Golden Horseshoe, an urban agglomeration of 9,765,188 people (as of 2021) surrounding the western end of Lake Ontario, while the Greater Toronto Area proper had a 2021 population of 6,712,341. Toronto is an international centre of business, finance, arts, sports, and culture and is one of the most multicultural and cosmopolitan cities in the world.\n",
      "Indigenous peoples have travelled through and inhabited the Toronto area, located on a broad sloping plateau interspersed with rivers, deep ravines, and urban forest, for more than 10,000 years. After the broadly disputed Toronto Purchase, when the Mississauga surrendered the area to the British Crown, the British established the town of York in 1793 and later designated it as the capital of Upper Canada. During the War of 1812, the town was the site of the Battle of York and suffered heavy damage by American troops. York was renamed and incorporated in 1834 as the city of Toronto. It was designated as the capital of the province of Ontario in 1867 during Canadian Confederation. The city proper has since expanded past its original limits through both annexation and amalgamation to its current area of 630.2 km2 (243.3 sq mi).\n",
      "The diverse population of Toronto reflects its current and historical role as an important destination for immigrants to Canada. About half of its residents were born outside of Canada and over 200 ethnic origins are represented among its inhabitants. While the majority of Torontonians speak English as their primary language, over 160 languages are spoken in the city. The mayor of Toronto is elected by direct popular vote to serve as the chief executive of the city. The Toronto City Council is a unicameral legislative body, comprising 25 councillors since the 2018 municipal election, representing geographical wards throughout the city.\n",
      "Toronto is a prominent centre for music, theatre, motion picture production, and television production, and is home to the headquarters of Canada's major national broadcast networks and media outlets. Its varied cultural institutions, which include numerous museums and galleries, festivals and public events, entertainment districts, national historic sites, and sports activities, attract over 43 million tourists each year. Toronto is known for its many skyscrapers and high-rise buildings, in particular the tallest free-standing structure on land outside of Asia, the CN Tower.\n",
      "The city is home to the Toronto Stock Exchange, the headquarters of Canada's five largest banks, and the headquarters of many large Canadian and multinational corporations. Its economy is highly diversified with strengths in technology, design, financial services, life sciences, education, arts, fashion, aerospace, environmental innovation, food services, and tourism. Toronto is the third-largest tech hub in North America after Silicon Valley and New York City, and the fastest growing hub.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24fdaa-334b-42e2-8458-49012d6213a5",
   "metadata": {},
   "source": [
    "## Creating a ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb2ea8-ce34-4c77-ae17-34a91573770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasoning and acting (ReAct)\n",
    "# agents = tools + chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "24c1148d-c5bd-4697-86e4-53660929ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchainhub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3bfa8ee9-4317-44be-a943-0518dda4d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_HUB_API_KEY=\"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a482fc13-4d14-43a8-9d0d-25d5c01a19f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "['agent_scratchpad', 'input', 'tool_names', 'tools']\n",
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the ChatOpenAI model (gpt-4-turbo-preview) with a temperature of 0. Utilize gpt-3.5-turbo if you use the free plan\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Define a template for answering questions\n",
    "template = '''\n",
    "Answer the following questions in Chinese as best you can.\n",
    "Questions: {q}\n",
    "'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# Pull the react prompt from the hub\n",
    "prompt = hub.pull('hwchase17/react', api_key=LANGCHAIN_HUB_API_KEY)\n",
    "\n",
    "# displaying information about the react prompt\n",
    "print(type(prompt))\n",
    "print(prompt.input_variables)\n",
    "print(prompt.template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "01ffef0f-b588-45e3-8cb8-a77de72c423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create three tools for the agent\n",
    "\n",
    "# 1. Python REPL Tool (for executing Python code)\n",
    "python_repl = PythonREPLTool()\n",
    "python_repl_tool = Tool(\n",
    "    name='Python REPL',\n",
    "    func=python_repl.run,\n",
    "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
    ")\n",
    "\n",
    "# 2. Wikipedia Tool (for searching Wikipedia)\n",
    "api_wrapper = WikipediaAPIWrapper()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wikipedia_tool = Tool(\n",
    "    name='Wikipedia',\n",
    "    func=wikipedia.run,\n",
    "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
    ")\n",
    "\n",
    "# 3. DuckDuckGo Search Tool (for general web searches)\n",
    "search = DuckDuckGoSearchRun()\n",
    "duckduckgo_tool = Tool(\n",
    "    name='DuckDuckGo Search',\n",
    "    func=search.run,\n",
    "    description='Useful for when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
    ")\n",
    "\n",
    "# Combine the tools into a list\n",
    "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
    "\n",
    "# Create a react agent with the ChatOpenAI model, tools, and prompt\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "\n",
    "# Initialize the AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "504c8ea8-09a7-4cac-9e35-1d1cf2b2e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Wikipedia to look up information about Napoleon Bonaparte's early life in Chinese.\n",
      "\n",
      "Action: Wikipedia\n",
      "Action Input: \"拿破仑·波拿巴早年生活\"\u001b[0m\u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\u001b[32;1m\u001b[1;3mI should try a DuckDuckGo search instead to find information about Napoleon Bonaparte's early life in Chinese.\n",
      "\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"拿破仑·波拿巴早年生活\"\u001b[0m\u001b[38;5;200m\u001b[1;3m探索位于地中海科西嘉岛的拿破仑·波拿巴的出生地阿雅克肖，以及他在法国历史上留下的遗产。 ... 拿破仑在阿雅克肖的早年生活. 公共领域，通过Wikimedia Commons. 1769年，拿破仑出生于法国科西嘉岛的阿雅克修镇。据报道，就在前一年，法国从意大利城邦热那亚 ... 波拿巴王朝（ Bonaparte ，1804-1814年，1815年，1852-1870年）是欧洲 法国一个王朝，1804年由第一执政拿破仑·波拿巴建立。. 拿破仑一世在1799年11月9日发动军事政变取得政权，结束法国大革命。 他其后经法国人民投票先后成为终身执政及皇帝，把法兰西第一共和国改制成法兰西第一帝国，其后经过一连串 ... 拿破崙三世（法語： Napoléon III ，1808年4月20日—1873年1月9日），即夏尔·路易·拿破仑·波拿巴（法語： Charles Louis Napoléon Bonaparte ），法兰西第二共和国唯一一位总统及法兰西第二帝国唯一一位皇帝，亦是拿破仑一世的侄子和继承人，亦是拿破仑一世的皇后約瑟 ... 一、早年生活. 拿破仑·波拿巴于1769年8月15日出生在法国科西嘉岛的一个小村庄，他是家中的第二个儿子。尽管他的家庭并不富裕，但拿破仑的父母都受过良好的教育，注重子女的教育。这种家庭背景让拿破仑在早年就展现出卓越的领导才能和野心。 波拿巴家族于17 世纪末搬进了这座房子的一部分，从此以他们的名字命名。 接着，他们试图一点一点地接管这整个房子，一层一层，甚至一个房间 ...\u001b[0m\u001b[32;1m\u001b[1;3mI now have enough information to answer the question.\n",
      "\n",
      "Final Answer: 拿破仑·波拿巴于1769年8月15日出生在法国科西嘉岛的一个小村庄，他是家中的第二个儿子。尽管他的家庭并不富裕，但拿破仑的父母都受过良好的教育，注重子女的教育。这种家庭背景让拿破仑在早年就展现出卓越的领导才能和野心。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
    "# question = 'Who is the current prime minister of the Canada?'\n",
    "question = 'Tell me about Napoleon Bonaparte early life'\n",
    "output = agent_executor.invoke({\n",
    "    'input': prompt_template.format(q=question)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc55246-e7c9-40ea-bd0c-ca2b83d77d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b575de1-4629-4ba5-bab3-3f6b22080b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b95aa-7f57-4767-a70a-7ef596e1e76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cf3ee-3aef-455c-98a8-9a24d6afcbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76eeda9-2a4f-469d-a453-90fe1b956e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
